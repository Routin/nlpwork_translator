{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangruiqing/anaconda3/envs/pmr3.6/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "# hugging face的分词器，github地址：https://github.com/huggingface/tokenizers\n",
    "from transformers import AutoTokenizer\n",
    "# 用于构建词典\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.functional import pad, log_softmax\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #读取数据1000w行，这个数据太大收敛速度极慢，跑完一轮需要10小时\n",
    "# def getEnZh(data='train',data_num=1000):\n",
    "#     #data_dum:读取数据数量，单位10000\n",
    "#     if data == 'train':\n",
    "#         en_open = open('./dataset/train.en',encoding='utf-8')\n",
    "#         zh_open = open('./dataset/train.zh',encoding='utf-8')\n",
    "#     else:\n",
    "#         print('待补充')\n",
    "#         return\n",
    "    \n",
    "#     s = int(1000/data_num)\n",
    "#     en = en_open.readlines()[::s]\n",
    "#     zh = zh_open.readlines()[::s]\n",
    "#     en_open.close()\n",
    "#     zh_open.close()\n",
    "\n",
    "#     print('数据读取成功，返回值(en:list,zh:list,length:int)')\n",
    "#     return en,zh,len(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据2w行\n",
    "def getEnZh(data='train'):\n",
    "    #读取训练或测试数据集文件\n",
    "    file_reader = []\n",
    "    if data == 'train':\n",
    "        file_open = open('./transdata_2w/train.txt',encoding='utf-8')\n",
    "    elif data == 'test':\n",
    "        file_open = open('./transdata_2w/dev.txt',encoding='utf-8')\n",
    "    else:\n",
    "        print('getEnZh获得错误参数，请检查')\n",
    "        return [],[]\n",
    "    try:\n",
    "        file_reader = file_open.readlines()\n",
    "        # print(file_reader.shape)\n",
    "    finally:\n",
    "        file_open.close()\n",
    "\n",
    "    train_data = []\n",
    "    for i in file_reader:\n",
    "        train_data.append(re.split(r'(?:[\\t\\n])', i))\n",
    "\n",
    "    zh = []\n",
    "    en = []\n",
    "    for i in train_data:\n",
    "        en.append(i[0])\n",
    "        zh.append(i[1])    \n",
    "\n",
    "    if data == 'train':\n",
    "        print('训练数据集加载成功。')\n",
    "    elif data == 'test':\n",
    "        print('测试数据集加载成功。')\n",
    "    return en,zh,len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tokenizer\n",
    "# def mytokenizer(data,lantype='en',length=64):\n",
    "#     # token = ['<bos>']\n",
    "#     token = []\n",
    "#     if lantype == 'en':\n",
    "#         for i in re.split(r'([,.?!;:\\'\\\"\\s])',data):\n",
    "#             if i not in [' ','']:\n",
    "#                 token.append(i.lower())\n",
    "#     elif lantype == 'zh':\n",
    "#         zh = list(jieba.cut(data))\n",
    "#         token.extend(zh)\n",
    "#     else:\n",
    "#         print('tokenizer获得错误参数，请检查。')\n",
    "#         return\n",
    "#     # token.append('<eos>')\n",
    "#     # while(len(token)<length):\n",
    "#     #     token.append('<pad>')\n",
    "#     # if len(token)>=length:\n",
    "#     #     token = token[:length]\n",
    "#     #     token[length-1] = '<eos>'\n",
    "#     return token\n",
    "\n",
    "# print(mytokenizer(\"I'm a English tokenizer.\",'en',16))\n",
    "# print(mytokenizer(\"我是中文分词器！\",'zh',16))\n",
    "# print(mytokenizer(\"This is a sentence over the limit of the length.\",'en',8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformers里的tokenizer基于bert\n",
    "#构造一个分词器\n",
    "class Mytokenizer():\n",
    "    def __init__(self) -> None:\n",
    "        self.entokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.zhtokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "        pass\n",
    "    def en(self,s:str):\n",
    "        return self.entokenizer.tokenize(s)\n",
    "    def zh(self,s:str):\n",
    "        return self.zhtokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'a', 'english', 'token', '##izer', 'basin', '##g', 'on', 'bert', '.'] ['我', '是', '中', '文', '分', '词', '器', '基', '于', '[UNK]', '！']\n"
     ]
    }
   ],
   "source": [
    "#实现分词器\n",
    "mytokenizer = Mytokenizer()\n",
    "te = mytokenizer.en(\"I'm a English tokenizer basing on Bert.\")\n",
    "tz = mytokenizer.zh(\"我是中文分词器基于Bert！\")\n",
    "print(te,tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词表类与函数，包括生成词表，存储词表，读入词表\n",
    "class Vocab:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    #词表生成器\n",
    "    def vocab_generator(self,en,zh,mytokenizer):\n",
    "        vocab_en = ['<bos>','<eos>','<pad>','<unk>']\n",
    "        vocab_zh = ['<bos>','<eos>','<pad>','<unk>']\n",
    "\n",
    "        for i in en:\n",
    "            token = mytokenizer.en(i)\n",
    "            for j in token:\n",
    "                if j not in vocab_en:\n",
    "                    vocab_en.append(j)\n",
    "        print('英文词表构建完成')\n",
    "\n",
    "        for i in zh:\n",
    "            token = mytokenizer.zh(i)\n",
    "            for j in token:\n",
    "                if j not in vocab_zh:\n",
    "                    vocab_zh.append(j)\n",
    "        \n",
    "        print('中文词表构建完成')\n",
    "        return vocab_en,vocab_zh\n",
    "\n",
    "    #词表存储器\n",
    "    def vocab_save(self,vocab:list,name='vocab.txt'):\n",
    "\n",
    "        path = './vocab/'+name\n",
    "        f=open(path,\"w\",encoding='utf-8')\n",
    "    \n",
    "        for line in vocab:\n",
    "            f.write(line+'\\n')\n",
    "        f.close()\n",
    "        print('词表存储成功：'+path)\n",
    "\n",
    "    #词表读取器\n",
    "    def vocab_loader(self,dir='./vocab/',name=['vocab_en.txt','vocab_zh.txt']):\n",
    "        fen = open(dir+name[0])\n",
    "        fzh = open(dir+name[1])\n",
    "\n",
    "        ren = fen.readlines()\n",
    "        rzh = fzh.readlines()\n",
    "        vocab_en=[]\n",
    "        vocab_zh=[]\n",
    "\n",
    "        for i in ren:\n",
    "            vocab_en.append(i.replace('\\n',''))\n",
    "        print('英文词表读取完成')\n",
    "\n",
    "        for i in rzh:\n",
    "            vocab_zh.append(i.replace('\\n',''))\n",
    "        print('中文词表加载完成')\n",
    "        \n",
    "        return vocab_en,vocab_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集加载成功。\n"
     ]
    }
   ],
   "source": [
    "#实现词表,耗时很久\n",
    "\n",
    "#1000w数据\n",
    "# en,zh,_ = getEnZh('train',20)\n",
    "#2w数据\n",
    "en,zh,_ = getEnZh()\n",
    "# vocab = Vocab()\n",
    "# vocab_en,vocab_zh = vocab.vocab_generator(en,zh,mytokenizer)\n",
    "\n",
    "# #存储词表\n",
    "# vocab.vocab_save(vocab_en,'vocab_en_2.txt')\n",
    "# vocab.vocab_save(vocab_zh,'vocab_zh_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文词表读取完成\n",
      "中文词表加载完成\n"
     ]
    }
   ],
   "source": [
    "#读取词表\n",
    "vocab = Vocab()\n",
    "# vocab_en,vocab_zh = vocab.vocab_loader()\n",
    "vocab_en,vocab_zh = vocab.vocab_loader(name=['vocab_en_2.txt','vocab_zh_2.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 138, 606, 457, 424, 65, 302, 1553, 306, 203, 383, 306, 1237, 338, 75, 30, 304, 1826, 383, 730, 10, 1742, 144, 65, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "#词表参数化方法\n",
    "dict_en = dict(enumerate(vocab_en))\n",
    "dict_zh = dict(enumerate(vocab_zh))\n",
    "\n",
    "en_getindex = dict(zip(dict_en.values(),dict_en.keys()))\n",
    "zh_getindex = dict(zip(dict_zh.values(),dict_zh.keys()))\n",
    "\n",
    "def get_index(data, lan):\n",
    "    idx = []\n",
    "    #<unk>是词表的第四个，序号为3\n",
    "    for i in data:\n",
    "        if i == '[UNK]':\n",
    "            idx.append(3)\n",
    "            continue\n",
    "        if lan == 'en' :\n",
    "            try:\n",
    "                idx.append(en_getindex[i])\n",
    "            except:\n",
    "                idx.append(3)\n",
    "        elif lan == 'zh':\n",
    "            try:\n",
    "                idx.append(zh_getindex[i])\n",
    "            except:\n",
    "                idx.append(3)\n",
    "        else:\n",
    "            print('get_index参数错误')\n",
    "            return\n",
    "    return idx\n",
    "\n",
    "print(get_index(mytokenizer.en(\"I am going to tell you my whole life, the life which did not really begin until the day I first saw you.字\"),'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset设计\n",
    "class transDataset(Dataset):\n",
    "    def __init__(self,datatype='train') -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.datatype = datatype\n",
    "        self.en,self.zh,self.data_num = self.load_data()\n",
    "        self.en_tokens = self.enloader()\n",
    "        self.zh_tokens = self.zhloader()\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.en_tokens[index], self.zh_tokens[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "    \n",
    "    def load_data(self):\n",
    "        # return getEnZh(self.datatype)\n",
    "        return en,zh,len(en)\n",
    "\n",
    "    def enloader(self):\n",
    "        ent = []\n",
    "        for i in self.en:\n",
    "            ent.append(get_index(mytokenizer.en(i),'en'))\n",
    "        return ent\n",
    "\n",
    "    def zhloader(self):\n",
    "        zht = []\n",
    "        for i in self.zh:\n",
    "            zht.append(get_index(mytokenizer.zh(i),'zh'))\n",
    "        return zht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They like to sing. 他们都喜欢唱歌。\n",
      "([108, 214, 457, 192, 5], [27, 45, 440, 292, 102, 258, 257, 5])\n",
      "句子总数： 21033\n"
     ]
    }
   ],
   "source": [
    "#加载dataset测试\n",
    "dataset = transDataset()\n",
    "print(en[2000],zh[2000])\n",
    "print(dataset.__getitem__(2000))\n",
    "print(\"句子总数：\",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate设计，整理数据，并返回src，tgt\n",
    "def collate_fn(batch):\n",
    "    # 对一个batch的输入进行处理\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    # 给tokens列表添加<bos>,<eos>,<pad>,这里将每句长度都变成64,算上首尾最多62个词。\n",
    "    for (_src,_tgt) in batch:\n",
    "        s = [0]\n",
    "        t = [0]\n",
    "        # 限制句子长度\n",
    "        _src = _src[:62]\n",
    "        for i in _src:\n",
    "            s.append(i)\n",
    "        s.append(1)\n",
    "        # 补pad\n",
    "        while len(s)<64:\n",
    "            s.append(2)\n",
    "        \n",
    "        _tgt = _tgt[:62]\n",
    "        for i in _tgt:\n",
    "            t.append(i)\n",
    "        t.append(1)\n",
    "        while len(t)<64:\n",
    "            t.append(2)\n",
    "\n",
    "        src_list.append(s)\n",
    "        tgt_list.append(t)\n",
    "    \n",
    "    src = torch.Tensor(src_list)\n",
    "    tgt = torch.Tensor(tgt_list)\n",
    "\n",
    "    # nn.Embedding只吃tenser.long类型\n",
    "    src = src.long()\n",
    "    tgt = tgt.long()\n",
    "    \n",
    "    # tgt_y是目标句子去掉第一个token，即去掉<bos>\n",
    "    tgt_y = tgt[:, 1:]\n",
    "    # tgt是目标句子去掉最后一个token\n",
    "    tgt = tgt[:, :-1]\n",
    "\n",
    "    # 计算本次batch要预测的token数\n",
    "    n_tokens = (tgt_y != 2).sum()\n",
    "    \n",
    "    return src, tgt, tgt_y, n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.size: torch.Size([128, 64])\n",
      "tgt.size: torch.Size([128, 63])\n",
      "tgt_y.size: torch.Size([128, 63])\n",
      "n_tokens: tensor(1364)\n"
     ]
    }
   ],
   "source": [
    "# dataloader设计与实现,batch_size设置\n",
    "batch_size = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# dataset = transDataset()\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "src, tgt, tgt_y, n_tokens = next(iter(train_loader))\n",
    "src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "# 展示shu'jushuju\n",
    "print(\"src.size:\", src.size())\n",
    "print(\"tgt.size:\", tgt.size())\n",
    "print(\"tgt_y.size:\", tgt_y.size())\n",
    "print(\"n_tokens:\", n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型设计-位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=64):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型设计-主体模型\n",
    "class TranslationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, src_vocab, tgt_vocab, dropout=0.1):\n",
    "        super(TranslationModel, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(len(src_vocab), d_model, padding_idx=2)\n",
    "        self.tgt_embedding = nn.Embedding(len(tgt_vocab), d_model, padding_idx=2)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(d_model, dropout=dropout, batch_first=True)\n",
    "        self.predictor = nn.Linear(d_model, len(tgt_vocab))\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size()[-1]).to(device)\n",
    "        src_key_padding_mask = TranslationModel.get_key_padding_mask(src)\n",
    "        tgt_key_padding_mask = TranslationModel.get_key_padding_mask(tgt)\n",
    "\n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        out = self.transformer(src, tgt,\n",
    "                               tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_key_padding_mask,\n",
    "                               tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_padding_mask(tokens):\n",
    "        return tokens == 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化器与损失函数\n",
    "model = TranslationModel(256, vocab_en, vocab_zh)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "class TranslationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TranslationLoss, self).__init__()\n",
    "        # 使用KLDivLoss\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = 2\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        true_dist = torch.zeros(x.size()).to(device)\n",
    "        # 将对应index的部分填充为1\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), 1)\n",
    "        # 找出<pad>部分，对于<pad>标签，全部填充为0，没有1，避免其参与损失计算。\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "\n",
    "        # 计算损失\n",
    "        return self.criterion(x, true_dist.clone().detach())\n",
    "\n",
    "criteria = TranslationLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#推理函数\n",
    "def translator(model,src:str):\n",
    "    src_tokens = mytokenizer.en(src)\n",
    "    src_list = torch.Tensor(get_index(src_tokens,'en')).unsqueeze(0).to(device).long()\n",
    "    tgt = torch.tensor([[0]]).to(device)\n",
    "\n",
    "    for i in range(64):\n",
    "        \n",
    "        out = model(src_list, tgt)\n",
    "        # 预测结果，因为只需要看最后一个词，所以取`out[:, -1]`\n",
    "        predict = model.predictor(out[:, -1])\n",
    "        # 找出最大值的index\n",
    "        y = torch.argmax(predict, dim=1)\n",
    "        # 和之前的预测结果拼接到一起\n",
    "        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)\n",
    "        # 如果为<eos>，说明预测结束，跳出循环\n",
    "        if y == 1:\n",
    "            break\n",
    "    # 将预测tokens拼起来\n",
    "    \n",
    "    outputs = ''\n",
    "    for i in tgt[0]:\n",
    "        outputs = outputs+vocab_zh[i]\n",
    "\n",
    "    return outputs.replace(\"<bos>\", \"\").replace(\"<eos>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练器\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "step = 0\n",
    "epochs = 100\n",
    "save_after_step = 2000\n",
    "\n",
    "model_dir =Path(\"./2w_model_dir\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for index, data in enumerate(train_loader):\n",
    "        # 生成数据\n",
    "        src, tgt, tgt_y, n_tokens = data\n",
    "        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 进行transformer的计算\n",
    "        out = model(src, tgt)\n",
    "        # 将结果送给最后的线性层进行预测\n",
    "        out = model.predictor(out)\n",
    "\n",
    "        \"\"\"\n",
    "        计算损失。由于训练时我们的是对所有的输出都进行预测，所以需要对out进行reshape一下。\n",
    "                我们的out的Shape为(batch_size, 词数, 词典大小)，view之后变为：\n",
    "                (batch_size*词数, 词典大小)。\n",
    "                而在这些预测结果中，我们只需要对非<pad>部分进行，所以需要进行正则化。也就是\n",
    "                除以n_tokens。\n",
    "        \"\"\"\n",
    "        loss = criteria(out.contiguous().view(-1, out.size(-1)), tgt_y.contiguous().view(-1)) / n_tokens\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(\"Epoch {}/{}\".format(epoch, epochs))\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        loop.update()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        del src\n",
    "        del tgt\n",
    "        del tgt_y\n",
    "\n",
    "        if step != 0 and step % save_after_step == 0:\n",
    "            torch.save(model, model_dir / f\"model_{step}.pt\")\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model, model_dir / f\"model_done.pt\")\n",
    "print('训练完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#效果测试\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = torch.load('./2w_model_dir/model_done.pt')\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(3):\n",
    "    j = random.randint(0,20000)\n",
    "    out = translator(model,en[j])\n",
    "    print('原文:',en[j])\n",
    "    print('预测:',out)\n",
    "    print('答案:',zh[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3ace8e35c7b14041b2fae01e1c9c881d0ed1a1c7d93eb41c8163ccbe72f3783"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
